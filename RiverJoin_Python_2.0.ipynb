{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 0.0: Create the output directory\n",
    "\n",
    "import os\n",
    "\n",
    "def create_gpkgs():\n",
    "    \"\"\"\n",
    "    Creates <base>/RiverJoin/output_gpkg/ next to this script (or in cwd if __file__ is undefined),\n",
    "    and ensures placeholder GeoPackage files exist there:\n",
    "      - TempPoints.gpkg\n",
    "      - TempReaches.gpkg\n",
    "      - TempReachesBuffers.gpkg\n",
    "      - TempReachesPointsPerp.gpkg\n",
    "      - FinalReaches.gpkg\n",
    "      - FinalReachesWithJoinedCommonID.gpkg\n",
    "\n",
    "    Returns a dict mapping each filename to its full path.\n",
    "    \"\"\"\n",
    "    # 1) # Determine base directory: script folder if running as a file, else cwd\n",
    "    try:\n",
    "        script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    except NameError:\n",
    "        script_dir = os.getcwd()\n",
    "\n",
    "    # 2) Build our output base directory under \"output\"\n",
    "    base_dir = os.path.join(script_dir, \"RiverJoin\", \"output\")\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "    # 3) Define the GeoPackage filenames\n",
    "    gpkg_names = [\n",
    "        \"TempPoints.gpkg\",\n",
    "        \"TempReaches.gpkg\",\n",
    "        \"TempReachesBuffers.gpkg\",\n",
    "        \"TempReachesPointsPerp.gpkg\",\n",
    "        \"FinalReaches.gpkg\",\n",
    "        \"FinalReaches_JoinedCommonID.gpkg\",\n",
    "        # \"FinalReaches_AggregatedCommonID.gpkg\"\n",
    "    ]\n",
    "\n",
    "    # 4) Ensure each file’s parent directory exists (all under base_dir)\n",
    "    paths = {}\n",
    "    for name in gpkg_names:\n",
    "        full_path = os.path.join(base_dir, name)\n",
    "        # base_dir already exists, but safe to call again\n",
    "        os.makedirs(os.path.dirname(full_path), exist_ok=True)\n",
    "        print(f\"Ensured existence of directory: {full_path}\")\n",
    "        paths[name] = full_path\n",
    "\n",
    "    return paths\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    gpkg_paths = create_gpkgs()\n",
    "    output_point_gpkg              = gpkg_paths[\"TempPoints.gpkg\"]\n",
    "    output_reach_gpkg              = gpkg_paths[\"TempReaches.gpkg\"]\n",
    "    output_reach_buffer_gpkg       = gpkg_paths[\"TempReachesBuffers.gpkg\"]\n",
    "    output_reach_point_perp_gpkg   = gpkg_paths[\"TempReachesPointsPerp.gpkg\"]\n",
    "    output_final_reach_gpkg        = gpkg_paths[\"FinalReaches.gpkg\"]\n",
    "    output_final_reach_joined_gpkg = gpkg_paths[\"FinalReaches_JoinedCommonID.gpkg\"]\n",
    "    # output_final_reach_aggregated_gpkg = gpkg_paths[\"FinalReaches_AggregatedCommonID.gpkg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from typing import Optional\n",
    "\n",
    "## Step 0.1: Clip target flowlines by a VPU boundary or, if none provided, by the envelope of join flowlines\n",
    "def clip_flowlines_by_boundary(\n",
    "    join_fl_gpkg: str,\n",
    "    join_fl_layer: str,\n",
    "    target_fl_gpkg: str,\n",
    "    target_fl_layer: str,\n",
    "    output_clip_gpkg: str,\n",
    "    output_clip_layer: str,\n",
    "    vpu_boundary_gpkg: Optional[str] = None,\n",
    "    vpu_boundary_layer: Optional[str] = None\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Clips the target flowline layer by a boundary polygon. If a VPU boundary is supplied,\n",
    "    uses that polygon; otherwise computes the bounding envelope of the join flowlines\n",
    "    and uses it as the clipping polygon. Writes the result into a GeoPackage.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    join_fl_gpkg : str\n",
    "        GeoPackage path containing the join flowlines (used to compute envelope if needed).\n",
    "    join_fl_layer : str\n",
    "        Layer name inside join_fl_gpkg.\n",
    "    target_fl_gpkg : str\n",
    "        GeoPackage path containing the target flowlines to clip.\n",
    "    target_fl_layer : str\n",
    "        Layer name inside target_fl_gpkg.\n",
    "    output_clip_gpkg : str\n",
    "        GeoPackage path where the clipped result will be written.\n",
    "    output_clip_layer : str\n",
    "        Layer name to use inside output_clip_gpkg for the clipped result.\n",
    "    vpu_boundary_gpkg : Optional[str]\n",
    "        GeoPackage path containing an explicit VPU boundary polygon.\n",
    "    vpu_boundary_layer : Optional[str]\n",
    "        Layer name inside vpu_boundary_gpkg.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        The path to output_clip_gpkg containing the new clipped layer.\n",
    "    \"\"\"\n",
    "    # 1. Read join flowlines (for envelope fallback)\n",
    "    join_gdf = gpd.read_file(join_fl_gpkg, layer=join_fl_layer)\n",
    "\n",
    "    # 2. Determine clipping boundary\n",
    "    if vpu_boundary_gpkg and vpu_boundary_layer:\n",
    "        boundary_gdf = gpd.read_file(vpu_boundary_gpkg, layer=vpu_boundary_layer)\n",
    "    else:\n",
    "        # Compute envelope of all join flowlines\n",
    "        combined = join_gdf.geometry.union_all()\n",
    "        envelope = combined.envelope\n",
    "        boundary_gdf = gpd.GeoDataFrame({'geometry': [envelope]}, crs=join_gdf.crs)\n",
    "\n",
    "    # 3. Read target flowlines\n",
    "    target_gdf = gpd.read_file(target_fl_gpkg, layer=target_fl_layer)\n",
    "\n",
    "    # 4. Reproject target to match boundary CRS if necessary\n",
    "    if target_gdf.crs != boundary_gdf.crs:\n",
    "        target_gdf = target_gdf.to_crs(boundary_gdf.crs)\n",
    "\n",
    "    # 5. Perform clipping\n",
    "    clipped = gpd.clip(target_gdf, boundary_gdf)\n",
    "\n",
    "    # 6. Write clipped result\n",
    "    clipped.to_file(output_clip_gpkg, layer=output_clip_layer, driver=\"GPKG\", mode='w')\n",
    "\n",
    "    # 7. Notify user of output\n",
    "    print(f\"Step 0.1: Clipped flowlines written to: {output_clip_gpkg}/{output_clip_layer}\")\n",
    "\n",
    "    return clipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 1: Extracts join flowlines whose midpoints lie within a buffer around target flowlines. \n",
    "\n",
    "import geopandas as gpd\n",
    "import os\n",
    "from shapely.geometry import LineString\n",
    "\n",
    "def extract_stream_network_within_sword_buffer(\n",
    "    huc_id: str,\n",
    "    join_fl_gpkg: str,\n",
    "    join_fl_layer: str,\n",
    "    target_fl_gpkg: str,\n",
    "    target_fl_layer: str,\n",
    "    unique_id: str,\n",
    "    output_point_gpkg: str,\n",
    "    output_reach_buffer_gpkg: str,\n",
    "    output_reach_gpkg: str,\n",
    "    buffer_distance: int = 100,  # meters, default buffer distance\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Reads join & target from GeoPackages,\n",
    "    buffers the target, selects midpoints within that buffer, and writes\n",
    "    both midpoints and extracted reaches into GeoPackages, appending layers.\n",
    "    \"\"\"\n",
    "    # Read join flowlines\n",
    "    join_fl = gpd.read_file(join_fl_gpkg, layer=join_fl_layer)\n",
    "    # Read target flowlines\n",
    "    target_fl = gpd.read_file(target_fl_gpkg, layer=target_fl_layer)\n",
    "\n",
    "    # Reprojection logic:\n",
    "    # 1) If target_fl is already projected → no operation.\n",
    "    # 2) Else if join_fl is projected → project target_fl to join_fl CRS.\n",
    "    # 3) Else → project target_fl to EPSG:3857.\n",
    "    if target_fl.crs.is_projected:\n",
    "        print(\"Step 1: Situation 1: target_fl CRS is projected; no reprojection needed.\")\n",
    "    # elif join_fl.crs.is_projected:\n",
    "    #     print(\"Step 1: Situation 2: join_fl CRS is projected; reprojecting target_fl to join_fl CRS.\")\n",
    "    #     target_fl = target_fl.to_crs(join_fl.crs)\n",
    "    else:\n",
    "        # compute UTM zone from target_fl extent\n",
    "        minx, miny, maxx, maxy = target_fl.total_bounds\n",
    "        midx, midy = (minx + maxx) / 2, (miny + maxy) / 2\n",
    "        zone = int((midx + 180) // 6) + 1\n",
    "        epsg_utm = 32600 + zone if midy >= 0 else 32700 + zone\n",
    "        target_fl = target_fl.to_crs(epsg=epsg_utm)\n",
    "        print(f\"Step 1: Situation 3: neither CRS is projected; reprojecting target_fl to UTM zone EPSG:{epsg_utm}\")\n",
    "\n",
    "    # Compute midpoints\n",
    "    def midpoint_of_line(geom):\n",
    "        if geom.geom_type == \"MultiLineString\":\n",
    "            line = list(geom.geoms)[0]\n",
    "        else:\n",
    "            line = geom\n",
    "        return line.interpolate(0.5, normalized=True)\n",
    "\n",
    "    midpoints = join_fl.geometry.apply(midpoint_of_line)\n",
    "    midpoints_gdf = gpd.GeoDataFrame(\n",
    "        {unique_id: join_fl[unique_id], \"orig_index\": join_fl.index},\n",
    "        geometry=midpoints,\n",
    "        crs=join_fl.crs\n",
    "    )\n",
    "\n",
    "    # Write midpoints to GeoPackage\n",
    "    mid_layer = f\"join_fl_midpoints_{huc_id}\"\n",
    "    midpoints_gdf.to_file(\n",
    "        output_point_gpkg,\n",
    "        layer=mid_layer,\n",
    "        driver=\"GPKG\",\n",
    "        mode=\"w\"\n",
    "    )\n",
    "\n",
    "    # Buffer the target flowlines\n",
    "    buffered = target_fl.copy()\n",
    "    buffered.geometry = buffered.geometry.buffer(buffer_distance)\n",
    "\n",
    "    # Write buffer to GeoPackage\n",
    "    buf_layer = f\"target_fl_buffer_{int(buffer_distance)}m\"\n",
    "    buffered.to_file(\n",
    "        output_reach_buffer_gpkg,\n",
    "        layer=buf_layer,\n",
    "        driver=\"GPKG\",\n",
    "        mode=\"w\"\n",
    "    )\n",
    "\n",
    "    # Select midpoints within buffer\n",
    "    selected = gpd.sjoin(midpoints_gdf, buffered, how=\"inner\", predicate=\"within\")\n",
    "\n",
    "    # Extract join flowlines by midpoint indices\n",
    "    extracted_indices = selected[\"orig_index\"].unique()\n",
    "    extracted = join_fl.loc[extracted_indices].copy()\n",
    "\n",
    "    # Write extracted reaches to GeoPackage\n",
    "    extracted_layer = f\"join_fl_extracted_reaches_{huc_id}\"\n",
    "    extracted.to_file(\n",
    "        output_reach_gpkg,\n",
    "        layer=extracted_layer,\n",
    "        driver=\"GPKG\",\n",
    "        mode=\"w\"\n",
    "    )\n",
    "\n",
    "    print(f\"    Extracted flowlines saved to {output_reach_gpkg}/{extracted_layer}\")\n",
    "    return extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 2: Reconstruct disconnected segments\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import fiona\n",
    "\n",
    "def reconstruct_disconnected_segments(\n",
    "    unique_id: str,\n",
    "    unique_id_nextDown: str,\n",
    "    extracted_layer: str,\n",
    "    join_fl_gpkg: str,\n",
    "    join_fl_layer: str,\n",
    "    huc_id: str,\n",
    "    output_reach_gpkg: str,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "\n",
    "    1) Reads the initially extracted network (extract_gdf) and the full join flowline network (join_fl).\n",
    "    2) Finds NextDownIDs (unique_id_nextDown) that do not appear as HydroIDs in the extracted set.\n",
    "    3) Traces upstream from each “disconnected end,” adding any missing reaches until hitting a node that is already in the extracted set or a dead-end.\n",
    "    4) Exports those “missing_reaches” to output_reach_gpkg/missing_reaches_{huc_id}.\n",
    "    5) Merges them back into the extracted network, writing the final reconstructed network to output_reach_gpkg/output_final_reach_gpkg_{huc_id}.\n",
    "    \n",
    "    Returns:\n",
    "        Path to the final reconstructed feature class GeoPackage.\n",
    "    \"\"\"\n",
    "    # Step 0: Read inputs\n",
    "    extract_gdf = gpd.read_file(output_reach_gpkg, layer=extracted_layer)\n",
    "    orig_gdf    = gpd.read_file(join_fl_gpkg, layer=join_fl_layer)\n",
    "\n",
    "    hydro_ids     = extract_gdf[unique_id].to_numpy()\n",
    "    nextdown_ids  = extract_gdf[unique_id_nextDown].to_numpy()\n",
    "\n",
    "    # Filter out null NextDownIDs\n",
    "    valid_mask     = pd.notnull(nextdown_ids)\n",
    "    nextdown_valid = nextdown_ids[valid_mask]\n",
    "\n",
    "    disconnected_ends = np.unique(nextdown_valid[~np.isin(nextdown_valid, hydro_ids)])\n",
    "    disconnected_segments = []\n",
    "\n",
    "    # Step 1: Trace downstream from each disconnected end\n",
    "    for end_id in disconnected_ends:\n",
    "        current_id     = end_id\n",
    "        missing_reaches = []\n",
    "\n",
    "        while pd.notnull(current_id):\n",
    "            match = orig_gdf[orig_gdf[unique_id] == current_id]\n",
    "            if match.empty:\n",
    "                break\n",
    "\n",
    "            if current_id not in hydro_ids:\n",
    "                missing_reaches.append(current_id)\n",
    "\n",
    "            # Advance downstream\n",
    "            current_id = match.iloc[0][unique_id_nextDown]\n",
    "            if pd.isnull(current_id) or (current_id in hydro_ids):\n",
    "                break\n",
    "\n",
    "        if missing_reaches:\n",
    "            disconnected_segments.append(missing_reaches)\n",
    "\n",
    "    # Step 2: Export missing reaches\n",
    "    all_missing_ids = [fid for seg in disconnected_segments for fid in seg]\n",
    "    if all_missing_ids:\n",
    "        missing_mask        = orig_gdf[unique_id].isin(all_missing_ids)\n",
    "        missing_reaches_gdf = orig_gdf[missing_mask].copy()\n",
    "    else:\n",
    "        missing_reaches_gdf = orig_gdf.iloc[0:0].copy()  # empty\n",
    "\n",
    "    missing_reaches_fc = f\"join_fl_traced_reaches_{huc_id}\"\n",
    "    missing_reaches_gdf.to_file(output_reach_gpkg, \n",
    "                                layer=missing_reaches_fc,\n",
    "                                driver=\"GPKG\",\n",
    "                                mode=\"w\")\n",
    "\n",
    "    # Step 3: Merge and write final\n",
    "    merged_gdf = gpd.GeoDataFrame(\n",
    "        pd.concat([extract_gdf, missing_reaches_gdf], ignore_index=True),\n",
    "        crs=extract_gdf.crs\n",
    "    )\n",
    "\n",
    "    join_fl_traced_extracted = f\"join_fl_traced_extracted_reaches_{huc_id}\"\n",
    "\n",
    "    # 1) If that layer already exists in the GeoPackage, delete it\n",
    "    existing = fiona.listlayers(output_reach_gpkg)\n",
    "    if join_fl_traced_extracted in existing:\n",
    "        fiona.remove(output_reach_gpkg, layer=join_fl_traced_extracted)\n",
    "        # print(f\"Removed existing layer '{join_fl_traced_extracted}' from {output_reach_gpkg}\")\n",
    "    \n",
    "    merged_gdf.to_file(output_reach_gpkg,\n",
    "                    layer=join_fl_traced_extracted,\n",
    "                    driver=\"GPKG\",\n",
    "                    mode=\"w\")\n",
    "\n",
    "    print(f\"Step 2: Saved reconstructed reaches to {output_reach_gpkg}/{join_fl_traced_extracted}\")\n",
    "    return merged_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 3: check the unnormal situations\n",
    "\n",
    "import geopandas as gpd\n",
    "import os\n",
    "\n",
    "def process_flowlines(\n",
    "    extract_stream_gpkg: str,\n",
    "    extract_layer: str,\n",
    "    traced_stream_gpkg: str,\n",
    "    traced_layer: str,\n",
    "    original_gpkg: str,\n",
    "    original_layer: str,\n",
    "    output_final_reach_gpkg: str,\n",
    "    huc_id: str\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Step 3: Identify extraction situation and handle “all_extracted” case.\n",
    "    \n",
    "    1) Reads the extracted (via buffer) and traced networks, plus the full original network from GeoPackages.\n",
    "    2) Counts features in each.\n",
    "    3) If extract count == 0 → no intersection.\n",
    "       If traced count == extract count → all extracted; (# no dedupe) and write to output_final_reach_gpkg.\n",
    "       Else → need upstream extraction.\n",
    "    Returns the situation string.\n",
    "    \"\"\"\n",
    "    # Read layers from GeoPackages\n",
    "    extract_gdf = (\n",
    "        gpd.read_file(extract_stream_gpkg, layer=extract_layer)\n",
    "        .drop_duplicates(subset=\"geometry\").reset_index(drop=True)\n",
    "    )\n",
    "    traced_gdf  = (\n",
    "        gpd.read_file(traced_stream_gpkg, layer=traced_layer)\n",
    "        .drop_duplicates(subset=\"geometry\").reset_index(drop=True)\n",
    "    )\n",
    "    \n",
    "    # orig_gdf    = (\n",
    "    # gpd.read_file(original_gpkg, layer=original_layer)\n",
    "    # .drop_duplicates(subset=\"geometry\").reset_index(drop=True)\n",
    "    # )\n",
    "\n",
    "    count_extract  = len(extract_gdf)\n",
    "    count_trace    = len(traced_gdf)\n",
    "    # count_original = len(orig_gdf)\n",
    "\n",
    "    if count_extract == 0:\n",
    "        situation = \"no_intersection\"\n",
    "        print(\"Step 3: There may be no intersection of extracted and target flowlines in this VPU.\")\n",
    "        return situation\n",
    "\n",
    "    # elif count_trace == count_extract:\n",
    "    #     situation = \"all_extracted\"\n",
    "\n",
    "    #     # Write the raw extracted GeoDataFrame without deduplication\n",
    "    #     output_layer = f\"join_fl_final_reaches_{huc_id}\"\n",
    "    #     extract_gdf.to_file(\n",
    "    #         output_final_reach_gpkg,\n",
    "    #         layer=output_layer,\n",
    "    #         driver=\"GPKG\",\n",
    "    #         mode=\"w\"\n",
    "    #     )\n",
    "\n",
    "    #     print(f\"Step 3: All flowlines extracted → {output_final_reach_gpkg} (layer: {output_layer})\")\n",
    "    #     return situation\n",
    "\n",
    "    else:\n",
    "        situation = \"extract_upstream_others\"\n",
    "        print(\"Step 3: Upstream and other flowlines still need extraction (proceed to Step 4).\")\n",
    "        return situation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 4: Extract upstream and other flowlines\n",
    "## Step 4.0.1: Create the spaced nodes along the flowlines\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, MultiLineString\n",
    "from pyproj import CRS\n",
    "\n",
    "def float_to_tag(x: float) -> str:\n",
    "    # use general format so small numbers don't go to scientific notation\n",
    "    s = f\"{x:g}\"\n",
    "    return s.replace(\".\", \"p\")\n",
    "\n",
    "def extract_spaced_nodes_projected(\n",
    "    target_reach_gpkg: str,\n",
    "    target_reach_layer: str,\n",
    "    target_reach_unique_id_field: str,\n",
    "    output_reach_point_perp_gpkg: str,\n",
    "    output_layer: str,\n",
    "    spacing: float = 200  # meters,\n",
    ") -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    1) Read `target_reach_layer` from `target_reach_gpkg`.\n",
    "    2) If geographic, reproject to a suitable UTM zone (meters);\n",
    "       otherwise stay in the native projected CRS.\n",
    "    3) Interpolate points every `spacing` meters along each segment,\n",
    "       dropping the overlapping boundary point.\n",
    "    4) Drop exact duplicates.\n",
    "    5) If we reprojected in step 2, reproject back to original CRS,\n",
    "       then write to `output_reach_point_perp_gpkg`/`output_layer`.\n",
    "    \"\"\"\n",
    "    # 1) Load and clean\n",
    "    lines = (\n",
    "        gpd.read_file(target_reach_gpkg, layer=target_reach_layer)\n",
    "           .dropna(subset=[\"geometry\"])\n",
    "           .explode(ignore_index=True)\n",
    "    )\n",
    "    orig_crs = lines.crs\n",
    "\n",
    "    # print(f\"Original CRS: {orig_crs}\")\n",
    "\n",
    "    # 2) Decide whether to reproject into UTM\n",
    "    if orig_crs.is_geographic:\n",
    "        # pick UTM from centroid\n",
    "        minx, miny, maxx, maxy = lines.total_bounds\n",
    "        midx, midy = (minx + maxx) / 2, (miny + maxy) / 2\n",
    "        zone = int((midx + 180)//6) + 1\n",
    "        epsg_utm = 32600 + zone if midy >= 0 else 32700 + zone\n",
    "        lines_proj = lines.to_crs(epsg=epsg_utm)\n",
    "    else:\n",
    "        # already in a projected CRS (assumed meters)\n",
    "        lines_proj = lines\n",
    "\n",
    "    # print(f\"Projected CRS: {lines_proj.crs}\")\n",
    "\n",
    "    # 3) Build spaced nodes in projected CRS\n",
    "    point_records = []\n",
    "    for row in lines_proj.itertuples():\n",
    "        geom = row.geometry\n",
    "        parts = geom.geoms if isinstance(geom, MultiLineString) else [geom]\n",
    "        for part_idx, seg in enumerate(parts):\n",
    "            L = seg.length\n",
    "            if L == 0:\n",
    "                continue\n",
    "            # distances [0, spacing, 2*spacing, …, L]\n",
    "            n_steps = max(1, int(np.ceil(L / spacing)))\n",
    "            dists = np.linspace(0, L, n_steps + 1)\n",
    "            if part_idx > 0:\n",
    "                # drop the duplicate 0 m point on subsequent parts\n",
    "                dists = dists[1:]\n",
    "            for d in dists:\n",
    "                pt = seg.interpolate(d)\n",
    "                point_records.append({\n",
    "                    \"reach_id\": getattr(row, target_reach_unique_id_field, None),\n",
    "                    \"geometry\": pt\n",
    "                })\n",
    "\n",
    "    pts_proj = gpd.GeoDataFrame(point_records, crs=lines_proj.crs)\n",
    "\n",
    "    # 4) Drop any exact duplicates\n",
    "    pts_proj = pts_proj.drop_duplicates(subset=\"geometry\").reset_index(drop=True)\n",
    "\n",
    "    # print(f\"Points original CRS: {pts_proj.crs}\")\n",
    "\n",
    "    os.makedirs(os.path.dirname(output_reach_point_perp_gpkg), exist_ok=True)\n",
    "    pts_proj.to_file(\n",
    "        filename=output_reach_point_perp_gpkg,\n",
    "        layer=output_layer,\n",
    "        driver=\"GPKG\"\n",
    "    )\n",
    "\n",
    "    print(f\"Step 4.0.1: Generated and wrote {len(pts_proj)} nodes (spacing={float_to_tag(spacing)}m) to {output_reach_point_perp_gpkg}/{output_layer}\")\n",
    "    return pts_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 4.0.2: Generate perpendicular lines with a specified length at each node\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, LineString\n",
    "\n",
    "def generate_perpendicular_line(node: Point, previous_node: Point, length: float) -> LineString:\n",
    "        \n",
    "    \"\"\"\n",
    "    Generate a line perpendicular to the segment formed by `previous_node` and `node`.\n",
    "    \n",
    "    Parameters:\n",
    "    - node: The current shapely Point\n",
    "    - previous_node: The previous shapely Point\n",
    "    - length: Total length of the perpendicular line (in coordinate units)\n",
    "    \n",
    "    Returns:\n",
    "    - A shapely LineString, or None if the two points coincide.\n",
    "    \"\"\"\n",
    "    x1, y1 = previous_node.x, previous_node.y\n",
    "    x2, y2 = node.x, node.y\n",
    "    dx, dy = x2 - x1, y2 - y1\n",
    "    # perpendicular vector\n",
    "    perp_dx, perp_dy = -dy, dx\n",
    "    norm = np.hypot(perp_dx, perp_dy)\n",
    "    if norm == 0:\n",
    "        return None\n",
    "    perp_dx /= norm\n",
    "    perp_dy /= norm\n",
    "    # endpoints\n",
    "    start = (node.x + perp_dx * length/2, node.y + perp_dy * length/2)\n",
    "    end   = (node.x - perp_dx * length/2, node.y - perp_dy * length/2)\n",
    "    return LineString([start, end])\n",
    "\n",
    "def extract_perpendicular_lines(\n",
    "    nodes_gpkg: str,\n",
    "    nodes_layer: str,\n",
    "    output_reach_point_perp_gpkg: str,\n",
    "    perp_line_length: float = 600,  # meters\n",
    "    spacing: float = 200  # meters,\n",
    ") -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    1) Read `nodes_layer` from `nodes_gpkg`.\n",
    "    2) Generate length-`perp_line_length` perpendiculars at each consecutive node pair.\n",
    "    3) Clean invalid or degenerate lines.\n",
    "    4) Write to `output_reach_point_perp_gpkg` (GeoPackage) under layer \"<nodes_layer>_perp\".\n",
    "    \"\"\"\n",
    "    # 1) Read & clean nodes\n",
    "    nodes = gpd.read_file(nodes_gpkg, layer=nodes_layer)\n",
    "    nodes = nodes.dropna(subset=[\"geometry\"])\n",
    "    nodes = nodes[~nodes.geometry.is_empty]\n",
    "\n",
    "    # 2) Build perpendiculars\n",
    "    perp_lines = []\n",
    "    for i in range(1, len(nodes)):\n",
    "        cur = nodes.geometry.iloc[i]\n",
    "        prev = nodes.geometry.iloc[i-1]\n",
    "        line = generate_perpendicular_line(cur, prev, perp_line_length)\n",
    "        if line is not None:\n",
    "            perp_lines.append(line)\n",
    "\n",
    "    perp_gdf = gpd.GeoDataFrame(geometry=perp_lines, crs=nodes.crs)\n",
    "\n",
    "    # 3) Clean invalid or zero-length lines\n",
    "    def valid_or_none(geom):\n",
    "        if geom is not None and geom.is_valid and np.isfinite(geom.length) and geom.length > 0:\n",
    "            return geom\n",
    "        return None\n",
    "\n",
    "    perp_gdf[\"geometry\"] = perp_gdf[\"geometry\"].apply(valid_or_none)\n",
    "    perp_gdf = perp_gdf.dropna(subset=[\"geometry\"]).reset_index(drop=True)\n",
    "\n",
    "    # 4) Write out to GeoPackage\n",
    "    perp_layer = f\"perp_lines_len{float_to_tag(perp_line_length)}_space{float_to_tag(spacing)}m_{huc_id}\"\n",
    "    perp_gdf.to_file(\n",
    "        filename=output_reach_point_perp_gpkg,\n",
    "        layer=perp_layer,\n",
    "        driver=\"GPKG\"\n",
    "    )\n",
    "    print(f\"Step 4.0.2: Saved {len(perp_gdf)} perpendicular lines to {output_reach_point_perp_gpkg}/{perp_layer}\")\n",
    "\n",
    "    return perp_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 4.1: Remove perpendicular lines those intersect with already extracted flowlines\n",
    "\n",
    "def remove_touching_perpendiculars(\n",
    "    output_reach_gpkg: str,\n",
    "    traced_layer: str,\n",
    "    output_reach_point_perp_gpkg: str,\n",
    "    perp_layer: str,\n",
    "    huc_id: str\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Pure-Python replacement for remove_touching_perpendiculars.\n",
    "\n",
    "    1) Reads the traced flowlines from output_reach_gpkg/traced_layer\n",
    "       and the perpendicular lines from output_reach_point_perp_gpkg/perp_layer.\n",
    "    2) Performs an “intersects” spatial join to find which perp lines touch the traced reaches.\n",
    "    3) Drops those intersecting lines.\n",
    "    4) Writes out the remaining perp lines to \n",
    "       output_reach_point_perp_gpkg/perp_lines_notcross_{huc_id}.\n",
    "\n",
    "    Returns:\n",
    "        The name of the layer \"perp_lines_notcross_{huc_id}\".\n",
    "    \"\"\"\n",
    "\n",
    "    # Read the traced flowlines\n",
    "    traced_gdf = gpd.read_file(output_reach_gpkg, layer=traced_layer)\n",
    "    # Read the perpendicular lines\n",
    "    perp_gdf   = gpd.read_file(output_reach_point_perp_gpkg, layer=perp_layer)\n",
    "\n",
    "    # Align CRS if needed\n",
    "    if traced_gdf.crs != perp_gdf.crs:\n",
    "        perp_gdf = perp_gdf.to_crs(traced_gdf.crs)\n",
    "\n",
    "    # Find intersecting perp lines\n",
    "    joined = gpd.sjoin(\n",
    "        perp_gdf,\n",
    "        traced_gdf,\n",
    "        how=\"inner\",\n",
    "        predicate=\"intersects\"\n",
    "    )\n",
    "    intersecting_idx = joined.index.unique()\n",
    "\n",
    "    # Drop intersecting lines\n",
    "    if len(intersecting_idx) > 0:\n",
    "        perp_notcross_gdf = perp_gdf.drop(index=intersecting_idx)\n",
    "    else:\n",
    "        perp_notcross_gdf = perp_gdf.copy()\n",
    "\n",
    "    # Define output layer name\n",
    "    output_layer = f\"perp_lines_notcross_{huc_id}\"\n",
    "\n",
    "    # Write remaining lines into the GeoPackage\n",
    "    perp_notcross_gdf.to_file(\n",
    "        output_reach_point_perp_gpkg,\n",
    "        layer=output_layer,\n",
    "        driver=\"GPKG\"\n",
    "    )\n",
    "\n",
    "    print(f\"Step 4.1: Wrote non‐crossing perpendicular lines to {output_reach_point_perp_gpkg}/{output_layer}\")\n",
    "    return output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 4.2: Spatial join between all original target flowlines and remaining perpendicular lines\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def perform_spatial_join_with_perp_and_filter(\n",
    "    huc_id: str,\n",
    "    original_gpkg: str,\n",
    "    original_layer: str,\n",
    "    perp_gpkg: str,\n",
    "    perp_notcross_layer: str,\n",
    "    unique_id: str,\n",
    "    output_reach_point_perp_gpkg: str,\n",
    "    min_join_count: int = 5,\n",
    "    strm_order_field: str = None,\n",
    "    min_strm_order: int = 2\n",
    ") -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    1. One‐to‐many spatial join (intersects).\n",
    "    2. Compute Join_Count for each original reach.\n",
    "    3. Collapse to one‐to‐one (first match).\n",
    "    4. Write joined (with Join_Count).\n",
    "    5. Filter by:\n",
    "         Join_Count >= min_join_count\n",
    "         AND <strm_order_field> > min_strm_order\n",
    "    6. Write filtered result.\n",
    "\n",
    "    Parameters:\n",
    "    - min_join_count:    minimum number of perp intersections to keep\n",
    "    - strm_order_field:  name of the stream order attribute in your dataset\n",
    "    - min_strm_order:    minimum value of strm_order_field to keep (strictly greater)\n",
    "    \"\"\"\n",
    "    # 1) Read inputs\n",
    "    orig_gdf = gpd.read_file(original_gpkg, layer=original_layer)\n",
    "    perp_gdf = gpd.read_file(perp_gpkg,    layer=perp_notcross_layer)\n",
    "\n",
    "    # 2) Align CRS\n",
    "    if orig_gdf.crs != perp_gdf.crs:\n",
    "        perp_gdf = perp_gdf.to_crs(orig_gdf.crs)\n",
    "\n",
    "    # 3) One‐to‐many join\n",
    "    joined_1toN = gpd.sjoin(\n",
    "        orig_gdf,\n",
    "        perp_gdf,\n",
    "        how=\"left\",\n",
    "        predicate=\"intersects\"\n",
    "    )\n",
    "    # restore original geometry if needed\n",
    "    if \"geometry_left\" in joined_1toN.columns:\n",
    "        joined_1toN = (\n",
    "            joined_1toN\n",
    "            .set_geometry(\"geometry_left\")\n",
    "            .drop(columns=[\"geometry_right\"], errors=\"ignore\")\n",
    "        )\n",
    "\n",
    "    # 4) keep only matched\n",
    "    joined_filtered = joined_1toN[joined_1toN[\"index_right\"].notna()].copy()\n",
    "\n",
    "    # 5) compute Join_Count per original index\n",
    "    counts = joined_filtered.groupby(joined_filtered.index).size()\n",
    "    joined_filtered[\"Join_Count\"] = joined_filtered.index.to_series().map(counts).astype(int)\n",
    "\n",
    "    # 6) collapse to first match per unique_id\n",
    "    joined_1to1 = joined_filtered.drop_duplicates(subset=unique_id, keep=\"first\")\n",
    "\n",
    "    # 7) write the joined-with-counts layer\n",
    "    layer1 = f\"orig_join_perp_notcross_intersected_{huc_id}\"\n",
    "    joined_1to1.to_file(\n",
    "        output_reach_point_perp_gpkg,\n",
    "        layer=layer1,\n",
    "        driver=\"GPKG\"\n",
    "    )\n",
    "\n",
    "    # 8) apply filter based on parameters\n",
    "    if strm_order_field:\n",
    "        filtered_gdf = joined_1to1[\n",
    "            (joined_1to1[\"Join_Count\"] >= min_join_count) &\n",
    "            (joined_1to1[strm_order_field]  >  min_strm_order)\n",
    "        ]\n",
    "    else:\n",
    "        filtered_gdf = joined_1to1[\n",
    "            joined_1to1[\"Join_Count\"] >= min_join_count\n",
    "        ]\n",
    "\n",
    "    # 9) write the filtered result\n",
    "    layer2 = f\"orig_join_perp_notcross_intersected_filtered_{huc_id}\"\n",
    "    filtered_gdf.to_file(\n",
    "        output_reach_point_perp_gpkg,\n",
    "        layer=layer2,\n",
    "        driver=\"GPKG\"\n",
    "    )\n",
    "\n",
    "    # Build a human‐readable filter description\n",
    "    if strm_order_field:\n",
    "        cond_txt = f\"Join_Count≥{min_join_count} & {strm_order_field}>{min_strm_order}\"\n",
    "    else:\n",
    "        cond_txt = f\"Join_Count≥{min_join_count}\"\n",
    "\n",
    "    print(\n",
    "        f\"Step 4.2: Wrote {len(filtered_gdf)} further found flowlines \"\n",
    "        f\"({cond_txt}) to {output_reach_point_perp_gpkg}/{layer2}\"\n",
    "    )\n",
    "\n",
    "    return filtered_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 4.3: Merge the extracted, traced, and intersected. And drop duplicates\n",
    "\n",
    "def merge_and_drop_duplicates(\n",
    "    join_fl_perp_intersected_filtered: str,\n",
    "    join_fl_traced_extracted: str,\n",
    "    original_gpkg: str,\n",
    "    original_layer: str,\n",
    "    output_reach_point_perp_gpkg: str,\n",
    "    output_reach_gpkg: str,\n",
    "    huc_id: str\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    1) Reads join_fl_perp_intersected_filtered and join_fl_traced_extracted into GeoDataFrames.\n",
    "    2) Merges them row-wise.\n",
    "    3) Drops duplicates by [\"LINKNO\",\"DSLINKNO\",\"DSContArea\"].\n",
    "    4) Drops any columns not in the original feature class (except geometry/ID fields).\n",
    "    5) Writes out the final deduplicated layer to \n",
    "       output_reach_gpkg/all_extracted_reaches_{huc_id}.\n",
    "\n",
    "    Returns:\n",
    "        Path to the final merged & deduplicated feature class.\n",
    "    \"\"\"\n",
    "    # 1) Load the two inputs\n",
    "    gdf1 = gpd.read_file(output_reach_gpkg, layer=join_fl_traced_extracted)\n",
    "    gdf2 = gpd.read_file(output_reach_point_perp_gpkg, layer=join_fl_perp_intersected_filtered)\n",
    "\n",
    "    # 2) Align CRS if needed\n",
    "    if gdf1.crs != gdf2.crs:\n",
    "        gdf2 = gdf2.to_crs(gdf1.crs)\n",
    "\n",
    "    # 3) Merge row-wise\n",
    "    merged_gdf = gpd.GeoDataFrame(\n",
    "        pd.concat([gdf1, gdf2], ignore_index=True),\n",
    "        crs=gdf1.crs\n",
    "    )\n",
    "\n",
    "    # 4) Drop duplicate flowlines\n",
    "    deduped_gdf = merged_gdf.drop_duplicates(subset=[\"geometry\"])\n",
    "\n",
    "    # 5) Read original schema to decide which extra fields to drop\n",
    "    orig_schema = gpd.read_file(original_gpkg, layer=original_layer)\n",
    "    original_fields = set(orig_schema.columns)\n",
    "\n",
    "    # Identify fields in deduped_gdf not present in original schema\n",
    "    fields_to_remove = [fld for fld in deduped_gdf.columns if fld not in original_fields]\n",
    "    if fields_to_remove:\n",
    "        deduped_gdf = deduped_gdf.drop(columns=fields_to_remove, errors=\"ignore\")\n",
    "\n",
    "    # 6) Write out to the specified GPKG\n",
    "    join_fl_merged = f\"join_fl_merged_{huc_id}\"\n",
    "    deduped_gdf.to_file(\n",
    "        filename=output_reach_gpkg,\n",
    "        layer=join_fl_merged,\n",
    "        driver=\"GPKG\"\n",
    "    )\n",
    "\n",
    "    print(f\"Step 4.3: Merged and deduplicated flowlines saved to {output_reach_gpkg}/{join_fl_merged}\")\n",
    "    return deduped_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 4.4: Reconstruct disconnected segments again\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import fiona\n",
    "\n",
    "def reconstruct_disconnected_segments_again(\n",
    "    unique_id: str,\n",
    "    unique_id_nextDown: str,\n",
    "    extracted_layer: str,\n",
    "    join_fl_gpkg: str,\n",
    "    join_fl_layer: str,\n",
    "    huc_id: str,\n",
    "    output_reach_gpkg: str,\n",
    "    output_final_reach_gpkg: str,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Pure-Python replacement for reconstruct_disconnected_segments.\n",
    "\n",
    "    1) Reads the initially extracted network (extract_gdf) and the full join flowline network (join_fl).\n",
    "    2) Finds NextDownIDs (unique_id_nextDown) that do not appear as HydroIDs in the extracted set.\n",
    "    3) Traces upstream from each “disconnected end,” adding any missing reaches until hitting a node that is already in the extracted set or a dead-end.\n",
    "    4) Exports those “missing_reaches” to output_reach_gpkg/missing_reaches_{huc_id}.\n",
    "    5) Merges them back into the extracted network, writing the final reconstructed network to output_final_reach_gpkg/output_final_reach_gpkg_{huc_id}.\n",
    "    \n",
    "    Returns:\n",
    "        Path to the final reconstructed feature class GeoPackage.\n",
    "    \"\"\"\n",
    "    # Step 0: Read inputs\n",
    "    extract_gdf = gpd.read_file(output_reach_gpkg, layer=extracted_layer)\n",
    "    orig_gdf    = gpd.read_file(join_fl_gpkg, layer=join_fl_layer)\n",
    "\n",
    "    hydro_ids     = extract_gdf[unique_id].to_numpy()\n",
    "    nextdown_ids  = extract_gdf[unique_id_nextDown].to_numpy()\n",
    "\n",
    "    # Filter out null NextDownIDs\n",
    "    valid_mask     = pd.notnull(nextdown_ids)\n",
    "    nextdown_valid = nextdown_ids[valid_mask]\n",
    "\n",
    "    disconnected_ends = np.unique(nextdown_valid[~np.isin(nextdown_valid, hydro_ids)])\n",
    "    disconnected_segments = []\n",
    "\n",
    "    # Step 1: Trace downstream from each disconnected end\n",
    "    for end_id in disconnected_ends:\n",
    "        current_id     = end_id\n",
    "        missing_reaches = []\n",
    "\n",
    "        while pd.notnull(current_id):\n",
    "            match = orig_gdf[orig_gdf[unique_id] == current_id]\n",
    "            if match.empty:\n",
    "                break\n",
    "\n",
    "            if current_id not in hydro_ids:\n",
    "                missing_reaches.append(current_id)\n",
    "\n",
    "            # Advance downstream\n",
    "            current_id = match.iloc[0][unique_id_nextDown]\n",
    "            if pd.isnull(current_id) or (current_id in hydro_ids):\n",
    "                break\n",
    "\n",
    "        if missing_reaches:\n",
    "            disconnected_segments.append(missing_reaches)\n",
    "\n",
    "    # Step 2: Export missing reaches\n",
    "    all_missing_ids = [fid for seg in disconnected_segments for fid in seg]\n",
    "    if all_missing_ids:\n",
    "        missing_mask        = orig_gdf[unique_id].isin(all_missing_ids)\n",
    "        missing_reaches_gdf = orig_gdf[missing_mask].copy()\n",
    "    else:\n",
    "        missing_reaches_gdf = orig_gdf.iloc[0:0].copy()  # empty\n",
    "\n",
    "    merged_missing_reaches_fc = f\"join_fl_merged_traced_reaches_{huc_id}\"\n",
    "    missing_reaches_gdf.to_file(output_reach_gpkg, \n",
    "                                layer=merged_missing_reaches_fc,\n",
    "                                driver=\"GPKG\",\n",
    "                                mode=\"w\")\n",
    "\n",
    "    # Step 3: Merge and write final\n",
    "    merged_gdf = gpd.GeoDataFrame(\n",
    "        pd.concat([extract_gdf, missing_reaches_gdf], ignore_index=True),\n",
    "        crs=extract_gdf.crs\n",
    "    )\n",
    "    merged_gdf = merged_gdf.drop_duplicates(subset=[\"geometry\"])\n",
    "\n",
    "    join_fl_final = f\"join_fl_final_reaches_{huc_id}\"\n",
    "\n",
    "    # # 1) If that layer already exists in the GeoPackage, delete it\n",
    "    # existing = fiona.listlayers(output_final_reach_gpkg)\n",
    "    # if join_fl_final in existing:\n",
    "    #     fiona.remove(output_final_reach_gpkg, layer=join_fl_final)\n",
    "    #     # print(f\"Removed existing layer '{join_fl_final}' from {output_final_reach_gpkg}\")\n",
    "    \n",
    "    merged_gdf.to_file(output_final_reach_gpkg,\n",
    "                    layer=join_fl_final,\n",
    "                    driver=\"GPKG\",\n",
    "                    mode=\"w\")\n",
    "\n",
    "    print(f\"Step 4.4: Saved reconstructed reaches to {output_final_reach_gpkg}/{join_fl_final}\")\n",
    "    return merged_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 5: Interactive display of the output flowlines\n",
    "\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "\n",
    "def join_output_interactive_display(\n",
    "    situation: str,\n",
    "    join_fl_gpkg: str,\n",
    "    join_fl_layer: str,\n",
    "    target_fl_gpkg: str,\n",
    "    target_fl_layer: str,\n",
    "    output_clip_layer: str,\n",
    "    output_reach_gpkg: str,\n",
    "    output_final_reach_gpkg: str,\n",
    "    output_reach_point_perp_gpkg: str,\n",
    "    target_reach_unique_id_field: str,\n",
    "    huc_id: str,\n",
    "    unique_id: str,\n",
    "    simplify_tolerance: float = 1000\n",
    "):\n",
    "    \"\"\"\n",
    "    Interactive display of flowlines by situation, with custom styling:\n",
    "      - target & clipped:   weight=6,   color=#73B2FF\n",
    "      - simplified join:    weight=0.5, color=#732600\n",
    "      - perp_notcross:       weight=1,   color=#000000\n",
    "      - traced:             weight=3, color=#005CE6\n",
    "      - perp_filtered:      weight=3, color=yellow\n",
    "      - final:              weight=3, color=#E64C00\n",
    "\n",
    "    Adds a layer control to toggle each layer on or off.\n",
    "    \"\"\"\n",
    "    # derive layer names\n",
    "    perp_notcross  = f\"perp_lines_notcross_{huc_id}\"\n",
    "    final_layer    = f\"join_fl_final_reaches_{huc_id}\"\n",
    "    perp_filtered  = f\"orig_join_perp_notcross_intersected_filtered_{huc_id}\"\n",
    "    traced_layer   = f\"join_fl_traced_extracted_reaches_{huc_id}\"\n",
    "\n",
    "    # read & simplify join flowlines\n",
    "    join_fl   = gpd.read_file(join_fl_gpkg, layer=join_fl_layer)\n",
    "    join_simp = join_fl.copy()\n",
    "    join_simp.geometry = join_simp.geometry.simplify(\n",
    "        tolerance=simplify_tolerance,\n",
    "        preserve_topology=True\n",
    "    )\n",
    "\n",
    "    # base styling for target & clipped\n",
    "    base_kwargs = dict(\n",
    "        color=\"#73B2FF\",\n",
    "        style_kwds={\"weight\": 6},\n",
    "        tooltip=target_reach_unique_id_field\n",
    "    )\n",
    "\n",
    "    # 1) no_intersection: target then join\n",
    "    if situation == \"no_intersection\":\n",
    "        m = gpd.read_file(target_fl_gpkg, layer=target_fl_layer).explore(\n",
    "            tiles=\"cartodbpositron\",\n",
    "            name=\"Target flowlines\",\n",
    "            **base_kwargs\n",
    "        )\n",
    "        join_simp.explore(\n",
    "            m=m,\n",
    "            name=\"Join (simplified)\",\n",
    "            color=\"#732600\",\n",
    "            style_kwds={\"weight\": 0.5},\n",
    "            tooltip=unique_id\n",
    "        )\n",
    "\n",
    "    # 2) all_extracted: clipped, join, final\n",
    "    elif situation == \"all_extracted\":\n",
    "        m = gpd.read_file(output_reach_gpkg, layer=output_clip_layer).explore(\n",
    "            tiles=\"cartodbpositron\",\n",
    "            name=\"Clipped target\",\n",
    "            **base_kwargs\n",
    "        )\n",
    "        join_simp.explore(\n",
    "            m=m,\n",
    "            name=\"Join (simplified)\",\n",
    "            color=\"#732600\",\n",
    "            style_kwds={\"weight\": 0.5},\n",
    "            tooltip=unique_id\n",
    "        )\n",
    "        gpd.read_file(output_final_reach_gpkg, layer=final_layer).explore(\n",
    "            m=m,\n",
    "            name=\"Final reaches\",\n",
    "            color=\"#E64C00\",\n",
    "            style_kwds={\"weight\": 3},\n",
    "            tooltip=unique_id\n",
    "        )\n",
    "\n",
    "    # 3) extract_upstream_others: clipped, join, perp_notcross, final, perp_filtered, traced\n",
    "    elif situation == \"extract_upstream_others\":\n",
    "        m = gpd.read_file(output_reach_gpkg, layer=output_clip_layer).explore(\n",
    "            tiles=\"cartodbpositron\",\n",
    "            name=\"Clipped target\",\n",
    "            **base_kwargs\n",
    "        )\n",
    "        join_simp.explore(\n",
    "            m=m,\n",
    "            name=\"Join (simplified)\",\n",
    "            color=\"#732600\",\n",
    "            style_kwds={\"weight\": 0.5},\n",
    "            tooltip=unique_id\n",
    "        )\n",
    "        gpd.read_file(output_reach_point_perp_gpkg, layer=perp_notcross).explore(\n",
    "            m=m,\n",
    "            name=\"Perp notcross\",\n",
    "            color=\"#000000\",\n",
    "            style_kwds={\"weight\": 1},\n",
    "            tooltip=False\n",
    "        )\n",
    "        gpd.read_file(output_final_reach_gpkg, layer=final_layer).explore(\n",
    "            m=m,\n",
    "            name=\"Final reaches\",\n",
    "            color=\"#E64C00\",\n",
    "            style_kwds={\"weight\": 3},\n",
    "            tooltip=unique_id\n",
    "        )\n",
    "        gpd.read_file(output_reach_point_perp_gpkg, layer=perp_filtered).explore(\n",
    "            m=m,\n",
    "            name=\"Perp filtered\",\n",
    "            color=\"yellow\",\n",
    "            style_kwds={\"weight\": 3},\n",
    "            tooltip=unique_id\n",
    "        )\n",
    "        gpd.read_file(output_reach_gpkg, layer=traced_layer).explore(\n",
    "            m=m,\n",
    "            name=\"Traced reaches\",\n",
    "            color=\"#005CE6\",\n",
    "            style_kwds={\"weight\": 3},\n",
    "            tooltip=unique_id\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown situation: {situation!r}\")\n",
    "\n",
    "    # add layer control\n",
    "    folium.LayerControl(collapsed=False).add_to(m)\n",
    "\n",
    "    # save the map\n",
    "    m.save(f\"output_interactive_map_{situation}_{huc_id}.html\")\n",
    "    print(f\"Step 5: Interactive map saved to {os.getcwd()}/output_interactive_map_{situation}_{huc_id}.html\")\n",
    "\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 6: Integrate all the functions and run\n",
    "\n",
    "from typing import Optional\n",
    "import os\n",
    "import geopandas as gpd\n",
    "\n",
    "def run_workflow(\n",
    "    huc_id: str,\n",
    "    join_fl_gpkg: str,\n",
    "    join_fl_layer: str,\n",
    "    target_fl_gpkg: str,\n",
    "    target_fl_layer: str,\n",
    "    unique_id: str,\n",
    "    unique_id_nextDown: str,\n",
    "    target_reach_unique_id_field: str,\n",
    "    *,\n",
    "    # ← these two are now optional—if None, clip uses join envelope\n",
    "    vpu_boundary_gpkg: Optional[str] = None,\n",
    "    vpu_boundary_layer: Optional[str] = None,\n",
    "    buffer_distance: int = 100, # meters\n",
    "    spacing: float = 200, # meters\n",
    "    perp_line_length: float = 600, # meters\n",
    "    min_join_count: int = 5,\n",
    "    strm_order_field: Optional[str] = None,\n",
    "    min_strm_order: int = 2,\n",
    "    simplify_tolerance: float = 1000\n",
    "):\n",
    "    \"\"\"\n",
    "    Full RiverJoin workflow. Mandatory arguments:\n",
    "      - join_fl_gpkg, join_fl_layer\n",
    "      - target_fl_gpkg, target_fl_layer\n",
    "      - join_fl: unique_id, unique_id_nextDown, strm_order_field\n",
    "      - target_reach_unique_id_field\n",
    "      - huc_id\n",
    "\n",
    "    Optional settings (passed by keyword only):\n",
    "      - buffer_distance (e.g. \\\"100 m\\\")\n",
    "      - spacing (float meters or None for fixed count)\n",
    "      - perp_line_length (float meters)\n",
    "      - min_join_count (int)\n",
    "      - min_strm_order (int)\n",
    "      - simplify_tolerance (float)\n",
    "    \"\"\"\n",
    "    # Step 0.1: Clip SWORD by join flowlines envelope\n",
    "    output_clip_layer = f\"SWORD_VPU_{huc_id}_ByFunction\"\n",
    "    clip_flowlines_by_boundary(\n",
    "        join_fl_gpkg=join_fl_gpkg,\n",
    "        join_fl_layer=join_fl_layer,\n",
    "        target_fl_gpkg=target_fl_gpkg,\n",
    "        target_fl_layer=target_fl_layer,\n",
    "        output_clip_gpkg=output_reach_gpkg,\n",
    "        output_clip_layer=output_clip_layer,\n",
    "        vpu_boundary_gpkg=vpu_boundary_gpkg,\n",
    "        vpu_boundary_layer=vpu_boundary_layer\n",
    "    )\n",
    "\n",
    "    # Step 1: Extract within buffer\n",
    "    extract_stream_network_within_sword_buffer(\n",
    "        huc_id                   = huc_id,\n",
    "        join_fl_gpkg             = join_fl_gpkg,\n",
    "        join_fl_layer            = join_fl_layer,\n",
    "        target_fl_gpkg           = output_reach_gpkg,\n",
    "        target_fl_layer          = output_clip_layer,\n",
    "        unique_id                = unique_id,\n",
    "        output_point_gpkg        = output_point_gpkg,\n",
    "        output_reach_buffer_gpkg = output_reach_buffer_gpkg,\n",
    "        output_reach_gpkg        = output_reach_gpkg,\n",
    "        buffer_distance          = buffer_distance\n",
    "    )\n",
    "    extracted_layer = f\"join_fl_extracted_reaches_{huc_id}\"\n",
    "\n",
    "    # Step 2: Reconstruct disconnected segments\n",
    "    reconstruct_disconnected_segments(\n",
    "        unique_id,\n",
    "        unique_id_nextDown,\n",
    "        extracted_layer,\n",
    "        join_fl_gpkg,\n",
    "        join_fl_layer,\n",
    "        huc_id,\n",
    "        output_reach_gpkg\n",
    "    )\n",
    "    join_fl_traced_extracted = f\"join_fl_traced_extracted_reaches_{huc_id}\"\n",
    "\n",
    "    # Step 3: Check for further extraction\n",
    "    situation = process_flowlines(\n",
    "        extract_stream_gpkg      = output_reach_gpkg,\n",
    "        extract_layer            = extracted_layer,\n",
    "        traced_stream_gpkg       = output_reach_gpkg,\n",
    "        traced_layer             = join_fl_traced_extracted,\n",
    "        original_gpkg            = join_fl_gpkg,\n",
    "        original_layer           = join_fl_layer,\n",
    "        output_final_reach_gpkg  = output_final_reach_gpkg,\n",
    "        huc_id                   = huc_id\n",
    "    )\n",
    "\n",
    "    # Step 4: Upstream‐other branch\n",
    "    if situation == \"extract_upstream_others\":\n",
    "        # 4.0.1: create spaced nodes\n",
    "        extract_spaced_nodes_projected(\n",
    "            target_reach_gpkg             = output_reach_gpkg,\n",
    "            target_reach_layer            = output_clip_layer,\n",
    "            target_reach_unique_id_field  = target_reach_unique_id_field,\n",
    "            output_reach_point_perp_gpkg  = output_reach_point_perp_gpkg,\n",
    "            output_layer                  = f\"sword_nodes_space{float_to_tag(spacing)}m_vpu{huc_id}\",\n",
    "            spacing                       = spacing\n",
    "        )\n",
    "        # 4.0.2: generate perpendiculars\n",
    "        extract_perpendicular_lines(\n",
    "            nodes_gpkg                   = output_reach_point_perp_gpkg,\n",
    "            nodes_layer                  = f\"sword_nodes_space{float_to_tag(spacing)}m_vpu{huc_id}\",\n",
    "            output_reach_point_perp_gpkg = output_reach_point_perp_gpkg,\n",
    "            perp_line_length             = perp_line_length,\n",
    "            spacing                      = spacing\n",
    "        )\n",
    "        perp_layer = f\"perp_lines_len{float_to_tag(perp_line_length)}_space{float_to_tag(spacing)}m_{huc_id}\"\n",
    "        \n",
    "        # 4.1: remove touching perpendiculars\n",
    "        remove_touching_perpendiculars(\n",
    "            output_reach_gpkg               = output_reach_gpkg,\n",
    "            traced_layer                    = join_fl_traced_extracted,\n",
    "            output_reach_point_perp_gpkg    = output_reach_point_perp_gpkg,\n",
    "            perp_layer                      = perp_layer,\n",
    "            huc_id                          = huc_id\n",
    "        )\n",
    "        perp_notcross_layer = f\"perp_lines_notcross_{huc_id}\"\n",
    "\n",
    "        # 4.2: spatial join & filter\n",
    "        perform_spatial_join_with_perp_and_filter(\n",
    "            huc_id                           = huc_id,\n",
    "            original_gpkg                    = join_fl_gpkg,\n",
    "            original_layer                   = join_fl_layer,\n",
    "            perp_gpkg                        = output_reach_point_perp_gpkg,\n",
    "            perp_notcross_layer              = perp_notcross_layer,\n",
    "            unique_id                        = unique_id,\n",
    "            output_reach_point_perp_gpkg     = output_reach_point_perp_gpkg,\n",
    "            min_join_count                   = min_join_count,\n",
    "            strm_order_field                 = strm_order_field,\n",
    "            min_strm_order                   = min_strm_order\n",
    "        )\n",
    "        join_fl_perp_intersected_filtered = f\"orig_join_perp_notcross_intersected_filtered_{huc_id}\"\n",
    "\n",
    "        # 4.3: merge & dedupe\n",
    "        merge_and_drop_duplicates(\n",
    "            join_fl_perp_intersected_filtered = join_fl_perp_intersected_filtered,\n",
    "            join_fl_traced_extracted          = join_fl_traced_extracted,\n",
    "            original_gpkg                     = join_fl_gpkg,\n",
    "            original_layer                    = join_fl_layer,\n",
    "            output_reach_point_perp_gpkg      = output_reach_point_perp_gpkg,\n",
    "            output_reach_gpkg                 = output_reach_gpkg,\n",
    "            huc_id                            = huc_id\n",
    "        )\n",
    "        join_fl_merged = f\"join_fl_merged_{huc_id}\"\n",
    "\n",
    "        # 4.4: reconnect segments again\n",
    "        reconstruct_disconnected_segments_again(\n",
    "            unique_id,\n",
    "            unique_id_nextDown,\n",
    "            join_fl_merged,\n",
    "            join_fl_gpkg,\n",
    "            join_fl_layer,\n",
    "            huc_id,\n",
    "            output_reach_gpkg,\n",
    "            output_final_reach_gpkg\n",
    "        )\n",
    "    else:\n",
    "        print(f\"No intersection for VPU {huc_id}. Skipping Step 4.\")\n",
    "\n",
    "    # Step 5: Interactive display\n",
    "    m = join_output_interactive_display(\n",
    "        situation,\n",
    "        join_fl_gpkg,\n",
    "        join_fl_layer,\n",
    "        target_fl_gpkg,\n",
    "        target_fl_layer,\n",
    "        output_clip_layer,\n",
    "        output_reach_gpkg,\n",
    "        output_final_reach_gpkg,\n",
    "        output_reach_point_perp_gpkg,\n",
    "        target_reach_unique_id_field,\n",
    "        huc_id,\n",
    "        unique_id,\n",
    "        simplify_tolerance             = simplify_tolerance\n",
    "    )\n",
    "\n",
    "    print(\"Workflow complete.\")\n",
    "\n",
    "    # return it so that a naked call also renders the map\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the next cell to do the spatial join of SWORD stream networks and the GEOGLOWS stream networks\n",
    "### [Specify your own layer paths and fields; for the other parameter, specify or leave them default]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example call:\n",
    "huc_id = \"715\"\n",
    "\n",
    "run_workflow(\n",
    "    huc_id                      = huc_id,\n",
    "    join_fl_gpkg                = f\"/Volumes/MacOS/UA/Dataset/GEOGLOWS/streams_{huc_id}_v2.gpkg\",\n",
    "    join_fl_layer               = f\"streams_{huc_id}\",\n",
    "    target_fl_gpkg              = \"/Volumes/MacOS/UA/Dataset/SWORD/SWORD_v17b_gpkg/na_sword_reaches_v17b.gpkg\",\n",
    "    target_fl_layer             = \"reaches\",\n",
    "    unique_id                   = \"LINKNO\",\n",
    "    unique_id_nextDown          = \"DSLINKNO\",\n",
    "    target_reach_unique_id_field= \"reach_id\",\n",
    "\n",
    "    ## Optional VPU boundary\n",
    "    ## If None, will use join flowlines envelope for clipping\n",
    "    # vpu_boundary_gpkg           = \"/Volumes/MacOS/UA/Dataset/RiverJoin/GDBs_GEOGLOWS_SWORD/OtherData.gpkg\",\n",
    "    # vpu_boundary_layer          = f\"GEOGLOWS_VPU_{huc_id}\",\n",
    "\n",
    "    ## Optional parameters\n",
    "    # buffer_distance             = 100,         # meters, buffer distance of the target flowlines in Step 1\n",
    "    # spacing                     = 200,         # meters, spacing for nodes in Step 4.0.1\n",
    "    # perp_line_length            = 600,         # meters, length of perpendicular lines in Step 4.0.2\n",
    "    # min_join_count              = 5,           # minimum number of intersections to keep in Step 4.2\n",
    "    strm_order_field            = \"strmOrder\",   # stream order field to filter by in Step 4.2\n",
    "    # min_strm_order              = 2,           # minimum stream order to keep in Step 4.2\n",
    "    # simplify_tolerance          = 1000         # meters, tolerance for simplifying geometries in Step 5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 7.0: Optional Function, if user calls compare_line_lengths, then execute it\n",
    "## Make it as a separate function, if the user call, then excute it, otherwise, just define it\n",
    "\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def pick_utm_epsg(gdf: gpd.GeoDataFrame) -> int:\n",
    "    \"\"\"Pick the appropriate UTM zone EPSG code from a layer’s centroid.\"\"\"\n",
    "    minx, miny, maxx, maxy = gdf.total_bounds\n",
    "    midx, midy = (minx + maxx) / 2, (miny + maxy) / 2\n",
    "    zone = int((midx + 180) // 6) + 1\n",
    "    return 32600 + zone if midy >= 0 else 32700 + zone\n",
    "\n",
    "def print_basic_stats(name: str, series) -> None:\n",
    "    \"\"\"\n",
    "    Print count, mean, std, min, 25%, 50%, 75%, max for the given numeric Series.\n",
    "    \"\"\"\n",
    "    stats = series.describe().round(2)\n",
    "    print(f\"\\n{name} (metres):\")\n",
    "    print(stats)\n",
    "\n",
    "def compare_line_lengths(\n",
    "    join_gpkg: str,\n",
    "    join_layer: str,\n",
    "    target_gpkg: str,\n",
    "    target_layer: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    1) Read two line layers from their GeoPackages.\n",
    "    2) Harmonize their CRS:\n",
    "       • If one is projected & the other geographic → reproject the geographic one.\n",
    "       • If both geographic → pick UTM from target & reproject both.\n",
    "       • If both projected → reproject join into target’s CRS.\n",
    "    3) Compute each line’s length in metres.\n",
    "    4) Print basic summary statistics for each set of lengths.\n",
    "    \"\"\"\n",
    "    # --- 1) Read layers ---\n",
    "    join_fl   = gpd.read_file(join_gpkg,   layer=join_layer)\n",
    "    target_fl = gpd.read_file(target_gpkg, layer=target_layer)\n",
    "\n",
    "    # --- 2) Harmonize CRS ---\n",
    "    crs1, crs2   = join_fl.crs,    target_fl.crs\n",
    "    geo1, geo2   = crs1.is_geographic, crs2.is_geographic\n",
    "\n",
    "    if not geo1 and geo2:\n",
    "        join_proj, target_proj = join_fl, target_fl.to_crs(crs1)\n",
    "    elif geo1 and not geo2:\n",
    "        join_proj, target_proj = join_fl.to_crs(crs2), target_fl\n",
    "    elif geo1 and geo2:\n",
    "        epsg_utm     = pick_utm_epsg(target_fl)\n",
    "        join_proj    = join_fl.to_crs(epsg=epsg_utm)\n",
    "        target_proj  = target_fl.to_crs(epsg=epsg_utm)\n",
    "    else:\n",
    "        join_proj, target_proj = join_fl.to_crs(crs2), target_fl\n",
    "\n",
    "    # --- 3) Compute lengths (metres in projected CRS) ---\n",
    "    join_lengths   = join_proj.geometry.length\n",
    "    target_lengths = target_proj.geometry.length\n",
    "\n",
    "    # --- 4) Print stats ---\n",
    "    print_basic_stats(\"Join line lengths\",   join_lengths)\n",
    "    print_basic_stats(\"Target line lengths\", target_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optionally run the next cell to have an overall impression of the lengths of the SWORD networks and the extracted GEOGLOWS ones\n",
    "### This may assist to decide how many nodes along your each flowline to use to join the corresponding joining flowline(s) (e.g. GEOGLOWS here) attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_clip_layer = f\"SWORD_VPU_{huc_id}_ByFunction\"\n",
    "join_fl_final = f\"join_fl_final_reaches_{huc_id}\"\n",
    "\n",
    "compare_line_lengths(\n",
    "    join_gpkg   = output_final_reach_gpkg,\n",
    "    join_layer  = join_fl_final,\n",
    "    target_gpkg = output_reach_gpkg,\n",
    "    target_layer= output_clip_layer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 7.1: Attributes translation from the join_fl to the target_fl\n",
    "## Step 7.1.1: Generate the nodes from the target flowlines\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, MultiLineString\n",
    "from shapely.ops import linemerge\n",
    "\n",
    "def extract_spaced_or_numbered_nodes_projected(\n",
    "    target_reach_gpkg: str,\n",
    "    target_reach_layer: str,\n",
    "    target_reach_unique_id_field: str,\n",
    "    output_final_reach_joined_gpkg: str,\n",
    "    output_layer: str,\n",
    "    spacing: float = None,\n",
    "    node_number: int = None\n",
    ") -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Read a line layer and interpolate nodes either by spacing or by fixed node count.\n",
    "    If neither provided, defaults to node_number=5. node_number must be ≥3.\n",
    "    \"\"\"\n",
    "\n",
    "    # 0) Default & validate modes/types\n",
    "    if spacing is None and node_number is None:\n",
    "        node_number = 5\n",
    "\n",
    "    if spacing is not None and node_number is not None:\n",
    "        raise ValueError(\n",
    "            f\"Specify exactly one of `spacing` or `node_number`, not both.\\n\"\n",
    "            f\"  spacing={spacing!r} (type: {type(spacing).__name__}), \"\n",
    "            f\"node_number={node_number!r} (type: {type(node_number).__name__}).\"\n",
    "        )\n",
    "\n",
    "    if spacing is not None and not isinstance(spacing, (int, float)):\n",
    "        raise TypeError(\n",
    "            f\"`spacing` must be a number (int or float), got {spacing!r} \"\n",
    "            f\"(type: {type(spacing).__name__}).\"\n",
    "        )\n",
    "\n",
    "    if node_number is not None:\n",
    "        if not isinstance(node_number, int):\n",
    "            raise TypeError(\n",
    "                f\"`node_number` must be an integer, got {node_number!r} \"\n",
    "                f\"(type: {type(node_number).__name__}).\"\n",
    "            )\n",
    "        if node_number < 3:\n",
    "            raise ValueError(\n",
    "                f\"`node_number` must be at least 3, got {node_number!r} \"\n",
    "                f\"(type: {type(node_number).__name__}).\"\n",
    "            )\n",
    "\n",
    "    # ... rest of function unchanged ...\n",
    "    lines = (\n",
    "        gpd.read_file(target_reach_gpkg, layer=target_reach_layer)\n",
    "           .dropna(subset=[\"geometry\"])\n",
    "        #    .explode(ignore_index=True)\n",
    "    )\n",
    "    orig_crs = lines.crs\n",
    "\n",
    "    # --- CHANGED: merge multipart geometries into single LineString per feature ---\n",
    "    lines[\"geometry\"] = lines.geometry.apply(\n",
    "        lambda g: linemerge(g) if isinstance(g, MultiLineString) else g\n",
    "    )\n",
    "\n",
    "    # Reproject to UTM if necessary\n",
    "    if orig_crs.is_geographic:\n",
    "        minx, miny, maxx, maxy = lines.total_bounds\n",
    "        midx, midy = (minx + maxx)/2, (miny + maxy)/2\n",
    "        zone = int((midx + 180)//6) + 1\n",
    "        epsg_utm = 32600 + zone if midy >= 0 else 32700 + zone\n",
    "        lines_proj = lines.to_crs(epsg=epsg_utm)\n",
    "        need_reproject_back = True\n",
    "    else:\n",
    "        lines_proj = lines\n",
    "        need_reproject_back = False\n",
    "\n",
    "    records = []\n",
    "    for row in lines_proj.itertuples():\n",
    "        seg = row.geometry  # always a single LineString now\n",
    "        L = seg.length\n",
    "        if L == 0:\n",
    "            continue\n",
    "\n",
    "        if spacing is not None:\n",
    "            q = L / spacing\n",
    "            n = int(np.floor(q)) + 1 if q >= 3 else 3\n",
    "        else:\n",
    "            n = node_number\n",
    "\n",
    "        fracs = np.linspace(0, 1, n)\n",
    "        for f in fracs:\n",
    "            pt = seg.interpolate(f * L)\n",
    "            records.append({\n",
    "                f\"{target_reach_unique_id_field}\": getattr(row, target_reach_unique_id_field, None),\n",
    "                \"geometry\": pt\n",
    "            })\n",
    "\n",
    "    pts_proj = gpd.GeoDataFrame(records, crs=lines_proj.crs)\n",
    "    pts = pts_proj.to_crs(orig_crs) if need_reproject_back else pts_proj\n",
    "\n",
    "    # os.makedirs(os.path.dirname(output_final_reach_joined_gpkg), exist_ok=True)\n",
    "    pts.to_file(\n",
    "        filename=output_final_reach_joined_gpkg,\n",
    "        layer=output_layer,\n",
    "        driver=\"GPKG\"\n",
    "    )\n",
    "\n",
    "    mode = \"spacing→nodes\" if spacing is not None else \"fixed nodes\"\n",
    "    print(f\"→ Generated and wrote {len(pts)} nodes ({mode}) \"\n",
    "          f\"to {output_final_reach_joined_gpkg}/{output_layer}\")\n",
    "    return pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 7.1.2: Join nearest flowline attributes to the nodes and then back to the target flowlines\n",
    "\n",
    "import os\n",
    "import geopandas as gpd\n",
    "from typing import List\n",
    "\n",
    "import os\n",
    "import geopandas as gpd\n",
    "\n",
    "def join_nearest_join_fl_attributes(\n",
    "    nodes_gpkg: str,\n",
    "    nodes_layer: str,\n",
    "    join_fl_gpkg: str,\n",
    "    join_fl_layer: str,\n",
    "    target_fl_gpkg: str,\n",
    "    target_fl_layer: str,\n",
    "    common_id: str,\n",
    "    output_gpkg: str,\n",
    "    output_nodes_layer: str,\n",
    "    output_flowlines_layer: str\n",
    ") -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    1) Read node points and flowlines.\n",
    "    2) Reproject join_fl into nodes CRS if they differ.\n",
    "    2b) Compute a 'reach_length_for_join' (in metres) on join_fl:\n",
    "         - if geographic, reproject to UTM, measure, then keep in original CRS\n",
    "         - else measure directly\n",
    "    3) Nearest‐geometry join: attach all flowline attributes to each node.\n",
    "    4) Drop the flowline geometry from the node table.\n",
    "    5) Join those node attributes back onto the target flowlines by `common_id`.\n",
    "    6) Write out:\n",
    "       - nodes_with_attrs layer to output_gpkg/output_nodes_layer\n",
    "       - enriched flowlines layer to output_gpkg/output_flowlines_layer\n",
    "       - enriched flowlines (no geometry) as CSV alongside the GPKG\n",
    "    \"\"\"\n",
    "    # 1) Load inputs\n",
    "    nodes     = gpd.read_file(nodes_gpkg,    layer=nodes_layer)\n",
    "    join_fl   = gpd.read_file(join_fl_gpkg,  layer=join_fl_layer)\n",
    "    target_fl = gpd.read_file(target_fl_gpkg, layer=target_fl_layer)\n",
    "\n",
    "    # 2) Align CRS if necessary\n",
    "    if join_fl.crs != nodes.crs:\n",
    "        join_fl = join_fl.to_crs(nodes.crs)\n",
    "    \n",
    "    # 2b) Compute reach_length_for_join (in metres)\n",
    "    aligned_crs = join_fl.crs\n",
    "    if aligned_crs.is_geographic:\n",
    "        # pick UTM zone based on join_fl extent\n",
    "        minx, miny, maxx, maxy = join_fl.total_bounds\n",
    "        midx, midy = (minx + maxx) / 2, (miny + maxy) / 2\n",
    "        zone = int((midx + 180) // 6) + 1\n",
    "        utm_epsg = 32600 + zone if midy >= 0 else 32700 + zone\n",
    "        tmp = join_fl.to_crs(epsg=utm_epsg)\n",
    "        join_fl['reach_length_for_join'] = tmp.geometry.length\n",
    "    else:\n",
    "        join_fl['reach_length_for_join'] = join_fl.geometry.length\n",
    "\n",
    "    # 3) Nearest‐geometry spatial join\n",
    "    joined_nodes = gpd.sjoin_nearest(\n",
    "        nodes,\n",
    "        join_fl,\n",
    "        how=\"left\",\n",
    "        distance_col=\"dist_to_line\"\n",
    "    )\n",
    "    joined_nodes = joined_nodes.drop(columns=\"geometry_right\", errors=\"ignore\")\n",
    "\n",
    "    # 4) Join node attributes back to the flowlines\n",
    "    node_attrs = joined_nodes.drop(columns=[\"geometry\"], errors=\"ignore\")\n",
    "    flowlines_with_attrs = target_fl.merge(\n",
    "        node_attrs,\n",
    "        on=common_id,\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # 6) Write out\n",
    "    os.makedirs(os.path.dirname(output_gpkg), exist_ok=True)\n",
    "    joined_nodes.to_file(\n",
    "        filename=output_gpkg,\n",
    "        layer=output_nodes_layer,\n",
    "        driver=\"GPKG\"\n",
    "    )\n",
    "    flowlines_with_attrs.to_file(\n",
    "        filename=output_gpkg,\n",
    "        layer=output_flowlines_layer,\n",
    "        driver=\"GPKG\"\n",
    "    )\n",
    "    # write CSV of flowlines attributes only\n",
    "    csv_dir = os.path.join(os.path.dirname(output_gpkg), \"FinalReaches_JoinedCommonID_csv\")\n",
    "    os.makedirs(csv_dir, exist_ok=True)\n",
    "    csv_path = os.path.join(csv_dir, f\"{output_flowlines_layer}.csv\")\n",
    "    flowlines_with_attrs.drop(columns=\"geometry\", errors=\"ignore\") \\\n",
    "        .to_csv(csv_path, index=False)\n",
    "\n",
    "    print(f\"→ Wrote {len(joined_nodes)} nodes to {output_gpkg}/{output_nodes_layer}\")\n",
    "    print(f\"→ Wrote {len(flowlines_with_attrs)} flowlines to {output_gpkg}/{output_flowlines_layer}\")\n",
    "    print(f\"→ Wrote CSV of flowlines attributes to {csv_path}\")\n",
    "\n",
    "    return flowlines_with_attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 7.2: Aggregate the joined flowlines to one per target flowline, based on the count of joined flowlines\n",
    "\n",
    "import os\n",
    "import geopandas as gpd\n",
    "\n",
    "def aggregate_joined_flowlines(\n",
    "    joined_output_gpkg: str,\n",
    "    joined_output_layer: str,\n",
    "    target_fl_unique_id_field: str,\n",
    "    join_fl_unique_id_field: str,\n",
    "    output_aggregated_layer: str\n",
    ") -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    1) Read `joined_output_layer` from `joined_output_gpkg`, containing multiple joined records per `target_fl_unique_id_field`.\n",
    "    2) Count how often each `join_fl_unique_id_field` occurs for each `target_fl_unique_id_field`.\n",
    "    3) Keep only the flowline with the highest count; tie-break by the 'reach_length_for_join' field.\n",
    "    4) Merge back full-feature records, drop duplicates.\n",
    "    5) Drop the 'reach_length_for_join' field.\n",
    "    6) Write aggregated layer back to GeoPackage under `output_aggregated_layer`.\n",
    "    7) Export attributes (no geometry) as CSV.\n",
    "    \"\"\"\n",
    "    # 1) Read the joined features\n",
    "    gdf = gpd.read_file(joined_output_gpkg, layer=joined_output_layer)\n",
    "\n",
    "    # 2) Count occurrences\n",
    "    counts = (\n",
    "        gdf\n",
    "        .groupby([target_fl_unique_id_field, join_fl_unique_id_field])\n",
    "        .size()\n",
    "        .reset_index(name='cnt')\n",
    "    )\n",
    "    counts['max_cnt'] = counts.groupby(target_fl_unique_id_field)['cnt'].transform('max')\n",
    "    tied = counts[counts['cnt'] == counts['max_cnt']].copy()\n",
    "\n",
    "    # 3) Tie-breaking by reach_length_for_join\n",
    "    lengths = (\n",
    "        gdf[[join_fl_unique_id_field, 'reach_length_for_join']]\n",
    "           .drop_duplicates(subset=join_fl_unique_id_field)\n",
    "    )\n",
    "    tied = tied.merge(lengths, on=join_fl_unique_id_field, how='left')\n",
    "    idx = tied.groupby(target_fl_unique_id_field)['reach_length_for_join'].idxmax()\n",
    "    chosen = tied.loc[idx, [target_fl_unique_id_field, join_fl_unique_id_field]]\n",
    "\n",
    "    # 4) Merge back and dedupe\n",
    "    aggregated = (\n",
    "        gdf\n",
    "        .merge(chosen, on=[target_fl_unique_id_field, join_fl_unique_id_field], how='inner')\n",
    "        .drop_duplicates(subset=[target_fl_unique_id_field, join_fl_unique_id_field])\n",
    "    )\n",
    "\n",
    "    # 5) Drop the helper length column\n",
    "    aggregated = aggregated.drop(columns=['reach_length_for_join'], errors='ignore')\n",
    "\n",
    "    # 6) Write aggregated layer\n",
    "    aggregated.to_file(\n",
    "        filename=joined_output_gpkg,\n",
    "        layer=output_aggregated_layer,\n",
    "        driver='GPKG',\n",
    "        mode='w'\n",
    "    )\n",
    "    print(f\"Aggregated {len(aggregated)} unique records \"\n",
    "          f\"to layer '{output_aggregated_layer}' in {joined_output_gpkg}\")\n",
    "\n",
    "    # 7) Export attributes to CSV\n",
    "    joined_output_csv_folder = os.path.join(os.path.dirname(joined_output_gpkg), \"FinalReaches_JoinedCommonID_csv\")\n",
    "    csv_path = os.path.join(joined_output_csv_folder, f\"{output_aggregated_layer}.csv\")\n",
    "    aggregated.drop(columns='geometry').to_csv(csv_path, index=False)\n",
    "    print(f\"Attribute table saved as CSV to {csv_path}\")\n",
    "\n",
    "    return aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7.2.1: Average aggregation, can only be used for numeric fields\n",
    "# TBD...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 8: Put all the data translation functions together\n",
    "\n",
    "import os\n",
    "import geopandas as gpd\n",
    "\n",
    "def process_final_node_join_aggregation(\n",
    "    huc_id: str,\n",
    "    output_final_reach_gpkg: str,\n",
    "    join_fl_final_layer: str,\n",
    "    output_reach_gpkg: str,\n",
    "    output_clip_layer: str,\n",
    "    output_final_reach_joined_gpkg: str,\n",
    "    target_fl_unique_id_field: str,\n",
    "    join_fl_unique_id_field: str,\n",
    "    spacing: float | None = None,\n",
    "    node_number: int | None = None\n",
    ") -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Runs the full three‐step sequence to:\n",
    "      1) Generate spaced (or numbered) nodes along the final reaches.\n",
    "      2) Nearest‐join flowline attributes to those nodes and back to reaches.\n",
    "      3) Aggregate to one record per reach by counting & longest‐reach tie‐break.\n",
    "    Returns the final aggregated GeoDataFrame.\n",
    "    \"\"\"\n",
    "    # 1) Generate nodes\n",
    "    nodes_layer = f\"join_fl_final_reaches_nodes_vpu{huc_id}\"\n",
    "    extract_spaced_or_numbered_nodes_projected(\n",
    "        target_reach_gpkg               = output_final_reach_gpkg,\n",
    "        target_reach_layer              = join_fl_final_layer,\n",
    "        target_reach_unique_id_field    = target_fl_unique_id_field,\n",
    "        output_final_reach_joined_gpkg  = output_final_reach_joined_gpkg,\n",
    "        output_layer                    = nodes_layer,\n",
    "        spacing                         = spacing,\n",
    "        node_number                     = node_number\n",
    "    )\n",
    "\n",
    "    # 2) Join nearest flowline attributes\n",
    "    joined_nodes_layer   = f\"join_fl_final_reaches_nodes_joinedAttr_vpu{huc_id}\"\n",
    "    joined_flowline_layer    = f\"join_fl_final_reaches_joinedAttr_vpu{huc_id}\"\n",
    "    join_nearest_join_fl_attributes(\n",
    "        nodes_gpkg           = output_final_reach_joined_gpkg,\n",
    "        nodes_layer          = nodes_layer,\n",
    "        join_fl_gpkg         = output_reach_gpkg,\n",
    "        join_fl_layer        = output_clip_layer,\n",
    "        target_fl_gpkg       = output_final_reach_gpkg,\n",
    "        target_fl_layer      = join_fl_final_layer,\n",
    "        common_id            = target_fl_unique_id_field,\n",
    "        output_gpkg          = output_final_reach_joined_gpkg,\n",
    "        output_nodes_layer   = joined_nodes_layer,\n",
    "        output_flowlines_layer = joined_flowline_layer\n",
    "    )\n",
    "\n",
    "    # 3) Aggregate to one reach record\n",
    "    aggregated_layer = f\"join_fl_final_reaches_joinedAttr_agg_vpu{huc_id}\"\n",
    "    aggregated = aggregate_joined_flowlines(\n",
    "        joined_output_gpkg            = output_final_reach_joined_gpkg,\n",
    "        joined_output_layer           = joined_flowline_layer,\n",
    "        target_fl_unique_id_field     = target_fl_unique_id_field,\n",
    "        join_fl_unique_id_field       = join_fl_unique_id_field,\n",
    "        output_aggregated_layer       = aggregated_layer\n",
    "    )\n",
    "\n",
    "    print(\"→ Completed node generation, join, and aggregation.\")\n",
    "    return aggregated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the next cell to do data transfer, here we join the unique id of corresponding GEOGLOWS flowline to the SWORD flowline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# huc_id = \"715\"\n",
    "\n",
    "# Determine base directory: script folder if running as a file, else cwd\n",
    "try:\n",
    "    script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    script_dir = os.getcwd()\n",
    "\n",
    "final_gdf = process_final_node_join_aggregation(\n",
    "    huc_id                        = huc_id,\n",
    "    output_final_reach_gpkg       = f\"{script_dir}/RiverJoin/output/TempReaches.gpkg\",\n",
    "    join_fl_final_layer           = f\"SWORD_VPU_{huc_id}_ByFunction\",\n",
    "    output_reach_gpkg             = f\"{script_dir}/RiverJoin/output/FinalReaches.gpkg\",\n",
    "    output_clip_layer             = f\"join_fl_final_reaches_{huc_id}\",\n",
    "    output_final_reach_joined_gpkg= f\"{script_dir}/RiverJoin/output/FinalReaches_JoinedCommonID.gpkg\",\n",
    "    target_fl_unique_id_field     = \"reach_id\",\n",
    "    join_fl_unique_id_field       = \"LINKNO\",\n",
    "\n",
    "    ## Optional parameters\n",
    "    # spacing                       = 1000,    # meters, or None if using node_number\n",
    "    # node_number                   = 5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
